{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>  MODEL STRIDE - FC1 -   KS 7 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d9661032b04dd584095e788082e35a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_ColormakerRegistry()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import os \n",
    "import glob\n",
    "import sys\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "#from utils import TrainingMetrics\n",
    "sys.path.insert(0, '/home/trz846/representation_learning/scripts/')\n",
    "import utils\n",
    "import time\n",
    "import preprocess_data\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import pearsonr\n",
    "from IPython.display import display, Markdown\n",
    "import nglview as nv\n",
    "import MDAnalysis as mda\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import representation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/trz846/representation_learning/models/cnn_strided_ls500_1fc_seq63/str2_ks7/'\n",
    "\n",
    "sys.path.insert(0, path)\n",
    "from cnn_seq_stride_FC import ConvNet\n",
    "kernel_size =  7\n",
    "stride = 2\n",
    "padding = 3\n",
    "ks_pool = None\n",
    "str_pool = None\n",
    "pad_pool = None\n",
    "\n",
    "# load architecture\n",
    "model = ConvNet(kernel_size, stride, padding)\n",
    "\n",
    "\n",
    "# load as checkpoint file \n",
    "trained_model_path = path +'checkpoint_model_epoch0_train_ds15.pt'\n",
    "# define optimizer as needed to load chckpt-file, not used \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "# load trained model weights into init model architectyre from file \n",
    "model, optimizer, epoch_start, train_ds, loss_list, acc_list = \\\n",
    "    utils.load_checkpoint(model, optimizer, filename=trained_model_path) \n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "# # transfer to gpu\n",
    "# if  torch.cuda.is_available(): \n",
    "#     model =  utils.load_final_model(path+'model_final.pt')\n",
    "#     model.cuda()\n",
    "# else:\n",
    "#     model =  torch.load(path+'model_final.pt', map_location=torch.device('cpu'))\n",
    "#     model.eval()\n",
    "\n",
    "# evaluation mode for model\n",
    "# model = model.eval\n",
    "# get encoder part only \n",
    "encoder = model.encoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_to_int = {\n",
    "        'M':1,\n",
    "        'R':2,\n",
    "        'H':3,\n",
    "        'K':4,\n",
    "        'D':5,\n",
    "        'E':6,\n",
    "        'S':7,\n",
    "        'T':8,\n",
    "        'N':9,\n",
    "        'Q':10,\n",
    "        'C':11,\n",
    "        'U':12, # Selenocystein. \n",
    "        'G':13,\n",
    "        'P':14,\n",
    "        'A':15,\n",
    "        'V':16,\n",
    "        'I':17,\n",
    "        'F':18,\n",
    "        'Y':19,\n",
    "        'W':20,\n",
    "        'L':21,\n",
    "        'O':22, # Pyrrolysine\n",
    "        'start':23,\n",
    "        'stop':24 }\n",
    "\n",
    "\n",
    "def fasta_2_input(fasta_file, max_l=498):\n",
    "        '''convert ddg's fasta file to a sequence that the CNN models\n",
    "        can read (like in preprocess_data.py )\n",
    "        '''\n",
    "\n",
    "        with open(fasta_file, 'r') as f:\n",
    "            seq = \"\"\n",
    "            for line in f:\n",
    "                # add to protein seq\n",
    "                if not line[0] == '>':\n",
    "                    seq += line.replace(\"\\n\",\"\")\n",
    "\n",
    "            # convert aa to integers        \n",
    "            int_seq = aa_seq_to_int(seq)\n",
    "            ## pad seq with 0's ##\n",
    "            int_seq = pad_seq(int_seq, max_l)\n",
    "\n",
    "        return np.array(int_seq)\n",
    "\n",
    "def aa_seq_to_int(seq):\n",
    "    \"\"\"\n",
    "    Return the sequence converted to integers as a list \n",
    "    plus start(23) and stop(24) integers. From Unirep. \n",
    "    \"\"\"\n",
    "    return [23] + [aa_to_int[aa] for aa in seq] + [24]\n",
    "    \n",
    "\n",
    "def pad_seq(seq, max_l):\n",
    "    '''\n",
    "    Pads the integer sequence with 0's up to max_l+2 \n",
    "    '''\n",
    "    padded_seq = [0]*(max_l+2) # +2 as start and stop added to seq \n",
    "    padded_seq[:len(seq)] = seq\n",
    "\n",
    "    return padded_seq\n",
    "\n",
    "    \n",
    "\n",
    "def get_repr_from_encoder(seq_input, encoder):\n",
    "    '''get AE encoders latent space on specific protein ''' \n",
    "    with torch.no_grad():\n",
    "        ### get WT representation ###\n",
    "        seq = torch.tensor(seq_input,dtype=torch.long)\n",
    "        if  torch.cuda.is_available(): \n",
    "            seq.cuda()\n",
    "        # convert to one hot \n",
    "        seq = utils.to_one_hot(seq)\n",
    "\n",
    "        # convert for channels to come first\n",
    "        seq = torch.transpose(seq,0,1) #transpose dim 1,2 => channels=aa\n",
    "\n",
    "        # add \"batch\" dimension=1 for model to work\n",
    "        seq = seq [None, :, :]\n",
    "        # get wt representation for seq from encoder model\n",
    "        repr = encoder(seq)\n",
    "        \n",
    "        return repr\n",
    "    \n",
    "def get_pred(seq_input, model):\n",
    "    '''get AE predictions on specific protein '''\n",
    "    with torch.no_grad():\n",
    "        ### get WT representation ###\n",
    "        seq_input = torch.tensor(seq_input,dtype=torch.long)\n",
    "        if  torch.cuda.is_available(): \n",
    "            seq_input=seq_input.cuda()\n",
    "        # convert to one hot \n",
    "        seq = utils.to_one_hot(seq_input)\n",
    "\n",
    "\n",
    "        # convert for channels to come first\n",
    "        seq = torch.transpose(seq,0,1) #transpose dim 1,2 => channels=aa\n",
    "\n",
    "        # add \"batch\" dimension=1 for model to work\n",
    "        seq = seq [None, :, :]\n",
    "\n",
    "        # get wt representation for seq from encoder model\n",
    "        out = model(seq)\n",
    "        \n",
    "        # get accuracy \n",
    "        _, predicted = torch.max(out.data, dim=1)\n",
    "        \n",
    "        \n",
    "        # get correct with pad\n",
    "        seq_input = seq_input[None, :]\n",
    "        correct_w_pad = (seq_input == predicted)\n",
    "        \n",
    "        # filter out padding (0)\n",
    "        msk = seq_input != 0\n",
    "        pred_msk = predicted[msk]\n",
    "        target_msk = seq_input[msk]\n",
    "        correct = (target_msk == pred_msk)\n",
    "        len_protein = msk.sum().item()\n",
    "        \n",
    "        return correct, correct_w_pad,len_protein\n",
    "\n",
    "\n",
    "def visual_prot(pdb,seq_input, model,cam_orient=None):\n",
    "    ''' visualises protein in cell output, where red in AE incorrect pred'''\n",
    "    # get MDanalysis stuff\n",
    "    u = mda.Universe(pdb, pdb)\n",
    "    protein = u.select_atoms(\"segid A\")\n",
    "    start_resid = protein.resids[0]\n",
    "    end_resid = protein.resids[-1]\n",
    "\n",
    "    # get nn model predictions for sequence\n",
    "    correct, correct_w_pad, len_protein = get_pred(seq_input, model)\n",
    "    correct_pos = correct\n",
    "\n",
    "    # CA = protein.select_atoms(\"name CA\")\n",
    "    # print('pdb',len(CA), 'start', start_resid, 'end',end_resid)\n",
    "\n",
    "    # add column for colouring the predictions\n",
    "    u.add_TopologyAttr('tempfactors')\n",
    "\n",
    "    # loop through all atoms in given resid and give same tempfactor according to prediction\n",
    "    temp_factors = []\n",
    "    for index,res in enumerate(u.atoms.residues):\n",
    "        resid = res.resid\n",
    "        pred = correct_pos[resid]\n",
    "        for atom in u.atoms.residues[index].atoms:\n",
    "            temp_factors.append(pred)\n",
    "    \n",
    "\n",
    "    # add b-factors\n",
    "    u.atoms.tempfactors = temp_factors\n",
    "    protein = u.select_atoms(\"segid A\") # only have chain A \n",
    "\n",
    "    # load universe object in nglview\n",
    "    t = nv.MDAnalysisTrajectory(protein)\n",
    "    w = nv.NGLWidget(t)\n",
    "\n",
    "    # define represention\n",
    "    w.representations = [\n",
    "        {\"type\": \"cartoon\", \"params\": {\n",
    "            \"sele\": \"protein\", \"color\": \"bfactor\"\n",
    "        }},\n",
    "        {\"type\": \"ball+stick\", \"params\": {\n",
    "            \"sele\": \"hetero\"\n",
    "        }}\n",
    "    ]\n",
    "    # w.download_image(filename='str2_ks5.png', factor=4, antialias=True, trim=False, transparent=True)\n",
    "    if cam_orient != None:\n",
    "        try: \n",
    "             w._set_camera_orientation(cam_orient)\n",
    "        except:\n",
    "            pass\n",
    "  \n",
    "    w._remote_call(\"setSize\", target=\"Widget\", args=[\"400px\", \"400px\"])\n",
    "\n",
    "    # use gui \n",
    "    # w.display(gui=True, use_box=False)\n",
    "\n",
    "    # show in dislay\n",
    "    return  w\n",
    "    \n",
    "    \n",
    "\n",
    "def repr_ddg(encoder, fn_fasta, fn_ddg, correct, lat_space=500):\n",
    "    seq_input = fasta_2_input(fn_fasta, max_l=498)\n",
    "    ddgs = pd.read_csv(fn_ddg, sep=\"\\s+\", header = None, \n",
    "                        names=['PDB', 'CHAIN', 'WT', 'RES', 'MUT', 'ddg'])\n",
    "\n",
    "    # define output  matrix\n",
    "    ls1 = lat_space # first :ls1 is wt representation\n",
    "    ls2 = lat_space*2 # ls1:ls2 mut representation\n",
    "    ls3 = lat_space*3 # ls2:ls3 = difference: wt repr - mut rep\n",
    "    ls4 = lat_space*3 + 500 # ls2:ls3 is wt labels seq()\n",
    "    ls5 = lat_space*3 + 2*500 # # ls2:ls3 is wt labels ()\n",
    "    col_pos = ls5 # col in matrix that defines of mutation in seq\n",
    "    col_wt  = ls5+1  # wt aa label \n",
    "    col_mut = ls5+2  # mut aa label \n",
    "    col_AE_pred = ls5+3 # column where AE pred are def\n",
    "    col_ddg = ls5+4 # column where exp ddgs are determine\n",
    "#    col_rf  = ls4+5 # column whree rand forrest are defined\n",
    "    \n",
    "    # initialise matrix, where rows = each mutation, and columns\n",
    "    ddgs_repr = np.zeros((len(ddgs),ls5+5))\n",
    "\n",
    "    ## get WT representation \n",
    "    wt_repr = get_repr_from_encoder(seq_input, encoder).cpu().numpy()\n",
    "\n",
    "    ## add wt_repr to first 500 columns in ddgs_repr\n",
    "    ddgs_repr[:,:ls1] = wt_repr\n",
    "\n",
    "    ## add WT labels to first columns between 1000:1500\n",
    "    ddgs_repr[:,ls3:ls4] = seq_input\n",
    "\n",
    "    for indx,row in ddgs.iterrows():\n",
    "\n",
    "        #create mut seq from wt seq\n",
    "        mut_seq = np.array(seq_input, copy=True) \n",
    "\n",
    "        # get seq index of mutation\n",
    "        mut_pos = row['RES'] # obs first (0) is startcodon so counts from 1\n",
    "\n",
    "        # wt/ original aa in int \n",
    "        wt_aa_int = utils.get_triAA_to_int(row['WT'])  \n",
    "\n",
    "        #  which aa its mutated to , convert mutation from tri code to int \n",
    "        mut_aa_int = utils.get_triAA_to_int(row['MUT'])\n",
    "\n",
    "        # change seq to include mutation \n",
    "        mut_seq[mut_pos] = mut_aa_int   # change aa in position \n",
    "    \n",
    "        ## get mutation repr from encoder\n",
    "        mut_repr = get_repr_from_encoder(mut_seq, encoder).cpu().numpy()\n",
    "\n",
    "        # add mut_repr to ddgs_repr\n",
    "        ddgs_repr[indx,ls1:ls2] = mut_repr\n",
    "        \n",
    "        # difference in representations\n",
    "        diff_repr = mut_repr - wt_repr\n",
    "        \n",
    "        # add diff_repr to ddg\n",
    "        ddgs_repr[indx, ls2:ls3] =  diff_repr\n",
    "        \n",
    "        # add mut fasta labels\n",
    "        ddgs_repr[indx, ls4:ls5] = mut_seq  \n",
    "\n",
    "        # save mut position\n",
    "        ddgs_repr[indx:, col_pos] = mut_pos\n",
    "\n",
    "        # save WT and mut aa label\n",
    "        ddgs_repr[indx:,col_wt]  =  wt_aa_int\n",
    "        ddgs_repr[indx:,col_mut] =  mut_aa_int\n",
    "\n",
    "        # track if autoencoder got pred of pos correct\n",
    "        if correct[mut_pos]:\n",
    "            ddgs_repr[indx:,col_AE_pred] = 1\n",
    "        else :\n",
    "            ddgs_repr[indx:,col_AE_pred] = 0\n",
    "\n",
    "    # add ddg measured values to last column in ddgs_repr\n",
    "    ddg_values =  np.array(ddgs['ddg'])\n",
    "    ddgs_repr[:,col_ddg]=ddg_values\n",
    "\n",
    "    ### Split in train and test set ###\n",
    "    # keep track of splitting by indexing \n",
    "    shuffl_indx = np.arange(len(ddgs))\n",
    "    np.random.shuffle(shuffl_indx)\n",
    "    train_len = int(0.80 * len(ddgs))\n",
    "    # random indexes\n",
    "    train_indx = shuffl_indx [:train_len]\n",
    "    test_indx = shuffl_indx [train_len:]\n",
    "\n",
    "    # get train/test from random indx\n",
    "    train_set = ddgs_repr[train_indx]\n",
    "    test_set = ddgs_repr[test_indx]\n",
    "    \n",
    "    # jaja could have returned pandas df but so much trouble to do so  \n",
    "    return  train_set, test_set\n",
    "\n",
    "\n",
    "def train_rand_forest(train_set, test_set, lat_space=500,\n",
    "                      params=None, n_trees = 120,  train_w_labels=False, train_w_labels_only = False,\n",
    "                      train_repr_diff=False):\n",
    "    \n",
    "    ls1 = lat_space # first :ls1 is wt representation\n",
    "    ls2 = lat_space*2 # ls1:ls2 mut representation\n",
    "    ls3 = lat_space*3 # ls2:ls3 = difference: wt repr - mut rep\n",
    "    ls4 = lat_space*3 + 500 # ls2:ls3 is wt labels seq()\n",
    "    ls5 = lat_space*3 + 2*500 # # ls2:ls3 is wt labels ()\n",
    "\n",
    "    # define x and y for training. \n",
    "    y_train =train_set[:,-1] \n",
    "    y_test = test_set[:,-1]\n",
    "\n",
    "    # which features to include in training together with repr\n",
    "    if train_w_labels:\n",
    "        x_train = np.concatenate((train_set[:,:ls2], train_set[:,ls3:ls5]),axis=1)\n",
    "        x_test  = np.concatenate((test_set[:,:ls2], test_set[:,ls3:ls5]),axis=1) \n",
    "    elif train_w_labels_only: # only use labels in training, no repr\n",
    "        x_train = train_set[:,ls4:ls5] \n",
    "        x_test  = test_set[:,ls4:ls5] \n",
    "    elif train_repr_diff: \n",
    "        x_train = train_set[:,ls2:ls3] \n",
    "        x_test  = test_set[:,ls2:ls3] \n",
    "    else: # only use representation\n",
    "        x_train = train_set[:,:ls2] \n",
    "        x_test  = test_set[:,:ls2] \n",
    "    \n",
    "    # init model using input params \n",
    "    if params is not None: \n",
    "        n_trees = params['n_estimators']\n",
    "        min_samples_split = params['min_samples_split']\n",
    "        min_samples_leaf = params['min_samples_leaf']\n",
    "        max_features = params['max_features']\n",
    "        max_depth = params['max_depth']\n",
    "        bootstrap = params['bootstrap']\n",
    "        rfr = RandomForestRegressor(n_estimators = n_trees, \n",
    "                                    min_samples_split=min_samples_split,\n",
    "                                    min_samples_leaf=min_samples_leaf,\n",
    "                                    max_features=max_features,\n",
    "                                    max_depth=max_depth,\n",
    "                                    bootstrap=bootstrap, oob_score=bootstrap)\n",
    "\n",
    "    else:     \n",
    "        rfr = RandomForestRegressor(n_estimators = n_trees, oob_score=True)\n",
    "    \n",
    "    # train model     \n",
    "    rfr.fit(x_train,y_train)\n",
    "    # get model prediction  on test set\n",
    "    y_pred = rfr.predict(x_test)\n",
    "    # add pred to last column of original matrix\n",
    "    test_set = np.column_stack((test_set, y_pred ))\n",
    " \n",
    "    # loss/error function (MSE):\n",
    "    rmsd = np.sqrt(np.mean((y_pred - y_test)**2 ))\n",
    "    pearson = pearsonr(y_pred, y_test)\n",
    "    \n",
    "    # split in AE correctly predicted and incorrectly predicted\n",
    "    AE_pred = test_set[:,-3] # col_AE_pred\n",
    "    idx_cor = np.argwhere(AE_pred==1)\n",
    "    idx_incor = np.argwhere(AE_pred==0)\n",
    "\n",
    "    return rfr, test_set, idx_cor, idx_incor, rmsd, pearson\n",
    "\n",
    "def CV_rand_forest(train_set, test_set, lat_space=500,\n",
    "                    train_w_labels=False, train_w_labels_only = False,\n",
    "                    train_repr_diff=False):\n",
    "    \n",
    "    ls1 = lat_space # first :ls1 is wt representation\n",
    "    ls2 = lat_space*2 # ls1:ls2 mut representation\n",
    "    ls3 = lat_space*3 # ls2:ls3 = difference: wt repr - mut rep\n",
    "    ls4 = lat_space*3 + 500 # ls2:ls3 is wt labels seq()\n",
    "    ls5 = lat_space*3 + 2*500 # # ls2:ls3 is wt labels ()\n",
    "\n",
    "    # define x and y for training. \n",
    "    y_train =train_set[:,-1] \n",
    "    y_test = test_set[:,-1]\n",
    "\n",
    "    # which features to include in training together with repr\n",
    "    if train_w_labels:\n",
    "        x_train = np.concatenate((train_set[:,:ls2], train_set[:,ls3:ls5]),axis=1)\n",
    "        x_test  = np.concatenate((test_set[:,:ls2], test_set[:,ls3:ls5]),axis=1) \n",
    "    elif train_w_labels_only: # only use labels in training, no repr\n",
    "        x_train = train_set[:,ls4:ls5] \n",
    "        x_test  = test_set[:,ls4:ls5] \n",
    "    elif train_repr_diff: \n",
    "        x_train = train_set[:,ls2:ls3] \n",
    "        x_test  = test_set[:,ls2:ls3] \n",
    "    else: # only use representation\n",
    "        x_train = train_set[:,:ls2] \n",
    "        x_test  = test_set[:,:ls2] \n",
    "        \n",
    "\n",
    "    # DEFINE FEATURES TO CV TEST \n",
    "    n_trees = np.arange(50,500,50)\n",
    "    # Number of features to consider at every split\n",
    "    max_feat = [ \"auto\",'sqrt','log2']\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "    max_depth.append(None)\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [2, 5, 10]\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "    # Method of selecting samples for training each tree\n",
    "    bootstrap = [True, False]\n",
    "\n",
    "    random_grid = {'n_estimators': n_trees,\n",
    "                   'max_features': max_feat,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "                   'bootstrap': bootstrap}\n",
    "    \n",
    "    ## Use the random grid to search for best hyperparameters\n",
    "    # First create the base model to tune\n",
    "    rf = RandomForestRegressor()\n",
    "    # Random search of parameters, using 3 fold cross validation, \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 200, verbose=2, random_state=42, n_jobs = -1)\n",
    "    #  Fit the random search model\n",
    "    rf_random.fit(x_train, y_train)\n",
    "    print ('BEST PARAMS:')\n",
    "    print(rf_random.best_params_)\n",
    "    best_params = rf_random.best_params_\n",
    "    \n",
    "    best_rf = rf_random.best_estimator_\n",
    "    y_pred = best_rf.predict(x_test)\n",
    "\n",
    "    #test_set = np.append(test_set, y_pred, axis=1 )\n",
    "    test_set = np.column_stack((test_set, y_pred ))\n",
    " \n",
    "    # loss/error function (MSE):\n",
    "    rmsd = np.sqrt(np.mean((y_pred - y_test)**2 ))\n",
    "    pearson = pearsonr(y_pred, y_test)\n",
    "    \n",
    "    # split in AE correctly predicted and incorrectly predicted\n",
    "    AE_pred = test_set[:,-3] # col_AE_pred\n",
    "    idx_cor = np.argwhere(AE_pred==1)\n",
    "    idx_incor = np.argwhere(AE_pred==0)\n",
    " \n",
    "    return best_rf,  test_set, idx_cor, idx_incor, rmsd, pearson, best_params\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasta\n",
    "fasta_file = '/home/trz846/representation_learning/data/ddgs/proteingmayo/raw/fasta/1PGA.fasta.txt'\n",
    "seq_input = fasta_2_input(fasta_file, max_l=498)\n",
    "pdb = \"/home/trz846/representation_learning/data/ddgs/proteingmayo/processed/pdbs/1PGA_clean.pdb\"\n",
    "\n",
    "# get decoder prediction \n",
    "correct, correct_w_pad, len_protein = get_pred(seq_input, model)\n",
    "acc =  correct.sum().item()/len_protein\n",
    "acc_pad =  correct_w_pad.sum().item()/len(seq_input) \n",
    "#print ('Accuracy for model on protein: ', correct.sum().item()/len(seq_input))\n",
    "Markdown('# G-PROTEIN MAYO \\n\\n ### Acc for model on protein: {:.3},  w padding: {:.3} '.format(acc, acc_pad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_file = '/home/trz846/representation_learning/data/ddgs/proteingmayo/raw/fasta/1PGA.fasta.txt'\n",
    "seq_input = fasta_2_input(fasta_file, max_l=498)\n",
    "pdb = \"/home/trz846/representation_learning/data/ddgs/proteingmayo/processed/pdbs/1PGA_clean.pdb\"\n",
    "# get decoder prediction \n",
    "correct, correct_w_pad, len_protein = get_pred(seq_input, model)\n",
    "cam_orient = [-50.41122655731412, -19.45342212415893, 0.03521294890622212, 0, -13.891423120996551, 36.06632237414191, 37.76211579590341, 0, -13.618562184236861, 35.220922664931315, -38.64906216158845, 0, -25.09212589263916, -24.426129817962646, -25.942466735839844, 1]\n",
    "w = visual_prot(pdb,seq_input, model,cam_orient)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest CV hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_fasta = '/home/trz846/representation_learning/data/ddgs/proteingmayo/raw/fasta/1PGA.fasta.txt'\n",
    "fn_ddg = '/home/trz846/representation_learning/data/ddgs/proteingmayo/processed/ddgs/proteingmayo.txt'\n",
    "seq_input = fasta_2_input(fn_fasta, max_l=498)\n",
    "correct, correct_w_pad, len_protein = get_pred(seq_input, model)\n",
    "\n",
    "\n",
    "#process data and get representation for seq, add in matrix with ddgs for training downstream models\n",
    "train_set, test_set  =  repr_ddg(encoder, fn_fasta, fn_ddg, correct)\n",
    "\n",
    "### train on difference in repres of mut andm wt\n",
    "# # get best params from random crossval\n",
    "base_rfr,  base_test_set, base_idx_cor, base_idx_incor, base_rmsd, base_pearson,best_params = CV_rand_forest(train_set, test_set, lat_space=500,\n",
    "                    train_w_labels=False, train_w_labels_only = False,\n",
    "                    train_repr_diff=True)\n",
    "\n",
    "# # ## representation best params from random CV \n",
    "# train downstream RF  using best params\n",
    "# best_params=\n",
    "rfr, rf_test_set, idx_cor, idx_incor, rmsd, pearson = train_rand_forest(train_set, test_set, \n",
    "                      lat_space=500, params=best_params, train_w_labels=False, train_w_labels_only = False,\n",
    "                        train_repr_diff=True)\n",
    "\n",
    "print('nr of features',rfr.n_features_ )\n",
    "print ('number of non_zeros in feature importance for repr; ', np.count_nonzero(rfr.feature_importances_))\n",
    "\n",
    "# # ### PLOTTING ### \n",
    "n_trees = best_params['n_estimators']\n",
    "fig, ax = plt.subplots(1, 2, sharey=False, figsize=(14, 7), constrained_layout=True) \n",
    "tst = 'Pearson: {:.2f} \\nRMSD: {:.2f}'.format(pearson[0], rmsd)\n",
    "\n",
    "ax[0].scatter(rf_test_set[:,-1][idx_cor], rf_test_set[:,-2][idx_cor], color='blue', label=tst)\n",
    "ax[0].scatter(rf_test_set[:,-1][idx_incor], rf_test_set[:,-2][idx_incor], color='red', label='AE incorrect pred')\n",
    "ax[0].legend(loc='upper left', fontsize=15)\n",
    "ax[0].set_xlabel('Predicted ∆∆G', fontsize=17)\n",
    "ax[0].set_ylabel(r'Experimental DMS  ', fontsize=17)\n",
    "ax[0].set_title(('Trained on representation \\nn_trees:{}, max_features:None').format(str(n_trees)), fontsize=20)\n",
    "ax[0].tick_params(axis='both', labelsize=16)\n",
    "ax[0].set_xlim(-2,4.1)\n",
    "ax[0].set_ylim(-2,4.1)\n",
    "ax[0].axhline(0, linewidth=2, color = 'k') \n",
    "ax[0].axvline(0, linewidth=2, color = 'k')\n",
    "ax[0].plot(ax[0].get_xlim(), ax[0].get_ylim(), ls=\"--\", c=\".3\")\n",
    "# Feature importance\n",
    "ax[1].bar(np.arange(rfr.n_features_),rfr.feature_importances_)\n",
    "ax[1].set_xlabel('latent space representation ', fontsize=17)\n",
    "ax[1].tick_params(axis='both', labelsize=16)\n",
    "ax[1].set_title('feature importance', fontsize=20)\n",
    "\n",
    "plt.savefig('plot_DMS_protein_g.pdf')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasta\n",
    "fasta_file = '/home/trz846/representation_learning/data/ddgs/dms_fowler/raw/fasta/1D5R.fasta.txt'\n",
    "seq_input = fasta_2_input(fasta_file, max_l=498)\n",
    "pdb = \"/home/trz846/representation_learning/data/ddgs/dms_fowler/processed/pdbs/1D5R_clean.pdb\"\n",
    "\n",
    "# get decoder prediction \n",
    "correct, correct_w_pad, len_protein = get_pred(seq_input, model)\n",
    "acc =  correct.sum().item()/len_protein\n",
    "acc_pad =  correct_w_pad.sum().item()/len(seq_input) \n",
    "#print ('Accuracy for model on protein: ', correct.sum().item()/len(seq_input))\n",
    "Markdown('# FOWLER - 1D5R \\n\\n ### Acc for model on protein: {:.3}, w padding: {:.3} '.format(acc, acc_pad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_file = '/home/trz846/representation_learning/data/ddgs/dms_fowler/raw/fasta/1D5R.fasta.txt'\n",
    "seq_input = fasta_2_input(fasta_file, max_l=498)\n",
    "correct, correct_w_pad, len_protein = get_pred(seq_input, model)\n",
    "pdb = \"/home/trz846/representation_learning/data/ddgs/dms_fowler/processed/pdbs/1D5R_clean.pdb\"\n",
    "cam_orient = [96.09886478780022, 13.7298922893142, -56.73615663350132, 0, -11.692529797584232, 111.59712514641404, 7.201363156325524, 0, 57.190787359066526, -0.2548374001420871, 96.80724210755506, 0, -33.43182325363159, -83.1609878540039, -31.10585927963257, 1]\n",
    "w = visual_prot(pdb,seq_input, model,cam_orient)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process fasta and  ddgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### baseline CV\n",
    "fn_fasta = '/home/trz846/representation_learning/data/ddgs/dms_fowler/raw/fasta/1D5R.fasta.txt'\n",
    "fn_ddg =  '/home/trz846/representation_learning/data/ddgs/dms_fowler/processed/ddgs/dms_fowler_1D5R.txt'\n",
    "seq_input = fasta_2_input(fn_fasta, max_l=498)\n",
    "correct, correct_w_pad, len_protein = get_pred(seq_input, model)\n",
    "\n",
    "\n",
    "#process data and get representation for seq, add in matrix with ddgs for training downstream models\n",
    "train_set, test_set  =  repr_ddg(encoder, fn_fasta, fn_ddg,correct)\n",
    "\n",
    "# train on difference in repres of mut andm wt\n",
    "# get best params from random crossval\n",
    "\n",
    "base_rfr,  base_test_set, base_idx_cor, base_idx_incor, base_rmsd, base_pearson, best_params = CV_rand_forest(train_set, test_set, lat_space=504,\n",
    "                    train_w_labels=False, train_w_labels_only = False,\n",
    "                    train_repr_diff=True)\n",
    "\n",
    "## representation best params from random CV \n",
    "# best_params=\n",
    "rfr, rf_test_set, idx_cor, idx_incor, rmsd, pearson = train_rand_forest(train_set, test_set, \n",
    "                      lat_space=500, params=best_params, train_w_labels=False, train_w_labels_only = False,\n",
    "                        train_repr_diff=True)\n",
    "print('nr of features',rfr.n_features_ )\n",
    "print ('non_zeros in feat import; ', np.count_nonzero(rfr.feature_importances_))\n",
    "\n",
    "\n",
    "# # ### PLOTTING ### \n",
    "n_trees = best_params['n_estimators']\n",
    "fig, ax = plt.subplots(1, 2, sharey=False, figsize=(14, 7), constrained_layout=True) \n",
    "tst = 'Pearson: {:.2f} \\nRMSD: {:.2f}'.format(pearson[0], rmsd)\n",
    "\n",
    "ax[0].scatter(rf_test_set[:,-1][idx_cor], rf_test_set[:,-2][idx_cor], color='blue', label=tst)\n",
    "ax[0].scatter(rf_test_set[:,-1][idx_incor], rf_test_set[:,-2][idx_incor], color='red', label='AE incorrect pred')\n",
    "ax[0].legend(loc='upper left', fontsize=15)\n",
    "ax[0].set_xlabel('Predicted ∆∆G', fontsize=17)\n",
    "ax[0].set_ylabel(r'Experimental DMS  ', fontsize=17)\n",
    "ax[0].set_title(('Trained on representation \\nn_trees:{}, max_features:None').format(str(n_trees)), fontsize=20)\n",
    "ax[0].tick_params(axis='both', labelsize=16)\n",
    "ax[0].set_xlim(-0.4,1.3)\n",
    "ax[0].set_ylim(-0.4,1.3)\n",
    "ax[0].axhline(0, linewidth=2, color = 'k') \n",
    "ax[0].axvline(0, linewidth=2, color = 'k')\n",
    "ax[0].plot(ax[0].get_xlim(), ax[0].get_ylim(), ls=\"--\", c=\".3\")\n",
    "# Feature importance\n",
    "ax[1].bar(np.arange(rfr.n_features_),rfr.feature_importances_)\n",
    "ax[1].set_xlabel('latent space representation ', fontsize=17)\n",
    "ax[1].tick_params(axis='both', labelsize=16)\n",
    "ax[1].set_title('feature importance', fontsize=20)\n",
    "\n",
    "plt.savefig('plot_DMS_1D5R.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasta\n",
    "fasta_file = '/home/trz846/representation_learning/data/ddgs/dms_fowler/raw/fasta/2H11.fasta.txt'\n",
    "seq_input = fasta_2_input(fasta_file, max_l=498)\n",
    "pdb = \"/home/trz846/representation_learning/data/ddgs/dms_fowler/processed/pdbs/2H11_clean.pdb\"\n",
    "\n",
    "# get decoder prediction \n",
    "correct, correct_w_pad, len_protein = get_pred(seq_input, model)\n",
    "acc =  correct.sum().item()/len_protein\n",
    "acc_pad =  correct_w_pad.sum().item()/len(seq_input) \n",
    "#print ('Accuracy for model on protein: ', correct.sum().item()/len(seq_input))\n",
    "Markdown('# FOWLER - 2H11 \\n\\n### Acc for model on protein: {:.3}, w padding: {:.3} '.format(acc, acc_pad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_file = '/home/trz846/representation_learning/data/ddgs/dms_fowler/raw/fasta/2H11.fasta.txt'\n",
    "seq_input = fasta_2_input(fasta_file, max_l=498)\n",
    "pdb = \"/home/trz846/representation_learning/data/ddgs/dms_fowler/processed/pdbs/2H11_clean.pdb\"\n",
    "# get decoder prediction \n",
    "correct, correct_w_pad, len_protein = get_pred(seq_input, model)\n",
    "cam_orient = [79.94508876962709, -0.12655908503161215, 11.267336805141511, 0, 0.1289010513754849, 80.73518092406466, -0.007742323550699695, 0, -11.267310255844327, 0.025655849540216713, 79.94518857093267, 0, -63.86358833312988, -28.813554525375366, -85.85820388793945, 1]\n",
    "w = visual_prot(pdb,seq_input, model,cam_orient)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # files to use\n",
    "fn_fasta = '/home/trz846/representation_learning/data/ddgs/dms_fowler/raw/fasta/2H11.fasta.txt'\n",
    "fn_ddg =  '/home/trz846/representation_learning/data/ddgs/dms_fowler/processed/ddgs/dms_fowler_2H11.txt'\n",
    "seq_input = fasta_2_input(fn_fasta, max_l=498)\n",
    "correct, correct_w_pad, len_protein = get_pred(seq_input, model)\n",
    "#process data and get representation for seq, add in matrix with ddgs for training downstream models\n",
    "train_set, test_set  =  repr_ddg(encoder, fn_fasta, fn_ddg,correct)\n",
    "\n",
    "# # train on difference in repres of mut andm wt\n",
    "# get best params from random crossval\n",
    "base_rfr,  base_test_set, base_idx_cor, base_idx_incor, base_rmsd, base_pearson,best_params = CV_rand_forest(train_set, test_set, lat_space=504,\n",
    "                    train_w_labels=False, train_w_labels_only = False,\n",
    "                    train_repr_diff=True)\n",
    "\n",
    "# # train downstream RF  using best params\n",
    "# best_params=\n",
    "rfr, rf_test_set, idx_cor, idx_incor, rmsd, pearson = train_rand_forest(train_set, test_set, \n",
    "                      lat_space=500, params=best_params, train_w_labels=False, train_w_labels_only = False,\n",
    "                        train_repr_diff=True)\n",
    "print('nr of features',rfr.n_features_ )\n",
    "print ('non_zeros in feat import; ', np.count_nonzero(rfr.feature_importances_))\n",
    "\n",
    "\n",
    "# # ### PLOTTING ### \n",
    "\n",
    "n_trees = best_params['n_estimators']\n",
    "fig, ax = plt.subplots(1, 2, sharey=False, figsize=(14, 7), constrained_layout=True) \n",
    "tst = 'Pearson: {:.2f} \\nRMSD: {:.2f}'.format(pearson[0], rmsd)\n",
    "\n",
    "ax[0].scatter(rf_test_set[:,-1][idx_cor], rf_test_set[:,-2][idx_cor], color='blue', label=tst)\n",
    "ax[0].scatter(rf_test_set[:,-1][idx_incor], rf_test_set[:,-2][idx_incor], color='red', label='AE incorrect pred')\n",
    "ax[0].legend(loc='upper left', fontsize=15)\n",
    "ax[0].set_xlabel('Predicted ∆∆G', fontsize=17)\n",
    "ax[0].set_ylabel(r'Experimental DMS  ', fontsize=17)\n",
    "ax[0].set_title(('Trained on representation \\nn_trees:{}, max_features:None').format(str(n_trees)), fontsize=20)\n",
    "ax[0].tick_params(axis='both', labelsize=16)\n",
    "ax[0].set_xlim(-0.4,1.3)\n",
    "ax[0].set_ylim(-0.4,1.3)\n",
    "ax[0].axhline(0, linewidth=2, color = 'k') \n",
    "ax[0].axvline(0, linewidth=2, color = 'k')\n",
    "ax[0].plot(ax[0].get_xlim(), ax[0].get_ylim(), ls=\"--\", c=\".3\")\n",
    "# Feature importance\n",
    "ax[1].bar(np.arange(rfr.n_features_),rfr.feature_importances_)\n",
    "ax[1].set_xlabel('latent space representation ', fontsize=17)\n",
    "ax[1].tick_params(axis='both', labelsize=16)\n",
    "ax[1].set_title('feature importance', fontsize=20)\n",
    "\n",
    "plt.savefig('plot_DMS_2H11.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
